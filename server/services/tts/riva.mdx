---
title: "NVIDIA Riva"
description: "Text-to-speech service implementation using NVIDIA Riva"
---

## Overview

NVIDIA Riva provides high-quality text-to-speech synthesis through cloud-based AI models accessible via gRPC API. The service offers multilingual support, configurable quality settings, and streaming audio generation optimized for real-time applications.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.riva.tts.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="NVIDIA Riva Docs"
    icon="book"
    href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html"
  >
    Official NVIDIA Riva TTS documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07r-interruptible-riva-nim.py"
  >
    Working example with Riva NIM
  </Card>
</CardGroup>

## Installation

To use NVIDIA Riva services, install the required dependencies:

```bash
pip install "pipecat-ai[riva]"
```

You'll also need to set up your NVIDIA API key as an environment variable: `NVIDIA_API_KEY`.

<Tip>
  Get your API key from the [NVIDIA Developer
  Portal](https://developer.nvidia.com/) and access to Riva services.
</Tip>

## Frames

### Input

- `TextFrame` - Text content to synthesize into speech
- `TTSSpeakFrame` - Text that should be spoken immediately
- `TTSUpdateSettingsFrame` - Runtime configuration updates
- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - LLM response boundaries

### Output

- `TTSStartedFrame` - Signals start of synthesis
- `TTSAudioRawFrame` - Generated audio data chunks (streaming)
- `TTSStoppedFrame` - Signals completion of synthesis
- `ErrorFrame` - API or processing errors

## Available Models

| Model                     | Description                            | Best For                              |
| ------------------------- | -------------------------------------- | ------------------------------------- |
| `magpie-tts-multilingual` | Multilingual model with natural voices | Conversational AI, multiple languages |
| `fastpitch-hifigan-tts`   | High-quality English synthesis         | English-only applications             |

<Note>
  The `magpie-tts-multilingual` model is the default and recommended for most
  use cases due to its multilingual capabilities and natural voice quality.
</Note>

## Language Support

The `magpie-tts-multilingual` model supports:

| Language Code    | Description      | Service Code |
| ---------------- | ---------------- | ------------ |
| `Language.EN_US` | English (US)     | `en-US`      |
| `Language.ES_US` | Spanish (US)     | `es-US`      |
| `Language.FR_FR` | French (France)  | `fr-FR`      |
| `Language.DE_DE` | German (Germany) | `de-DE`      |
| `Language.IT_IT` | Italian (Italy)  | `it-IT`      |
| `Language.ZH_CN` | Chinese (China)  | `zh-CN`      |

## Usage Example

### Basic Configuration

Initialize the Riva TTS service with your API key and desired voice:

```python
from pipecat.services.riva.tts import RivaTTSService
from pipecat.transcriptions.language import Language
import os

# Configure with default multilingual model
tts = RivaTTSService(
    api_key=os.getenv("NVIDIA_API_KEY"),
    voice_id="Magpie-Multilingual.EN-US.Ray",
    params=RivaTTSService.InputParams(
        language=Language.EN_US,
        quality=20
    )
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

### Dynamic Configuration

Make settings updates by pushing a `TTSUpdateSettingsFrame` for the `RivaTTSService`:

```python
from pipecat.frames.frames import TTSUpdateSettingsFrame

await task.queue_frame(TTSUpdateSettingsFrame(
    voice_id="Magpie-Multilingual.ES-US.Luna",
    params=RivaTTSService.InputParams(
        language=Language.ES_US,
    )
 ))
```

## Metrics

The service provides comprehensive metrics:

- **Time to First Byte (TTFB)** - Latency from text input to first audio
- **Processing Duration** - Total synthesis time
- **Character Usage** - Text processed for billing

<Info>
  [Learn how to enable Metrics](/guides/fundamentals/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Model Set at Initialization**: Models cannot be changed after initialization - configure `model_function_map` during construction
- **Deprecated Classes**: `FastPitchTTSService` is deprecated - use `RivaTTSService` instead
- **Quality vs Speed**: Higher quality settings increase synthesis time but improve audio quality
