---
title: "Azure"
description: "Text-to-speech service using Azure Cognitive Services Speech SDK"
---

## Overview

Azure Cognitive Services provides high-quality text-to-speech synthesis with two service implementations: `AzureTTSService` (WebSocket-based) for real-time streaming with low latency, and `AzureHttpTTSService` (HTTP-based) for batch synthesis. `AzureTTSService` is recommended for interactive applications requiring streaming capabilities.

<CardGroup cols={2}>
  <Card
    title="Azure TTS API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.azure.tts.html"
  >
    Pipecat's API methods for Azure TTS integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07f-interruptible-azure.py"
  >
    Complete example with streaming synthesis
  </Card>
  <Card
    title="Azure Speech Documentation"
    icon="book"
    href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/"
  >
    Official Azure Speech Services documentation
  </Card>
  <Card
    title="Voice Gallery"
    icon="microphone"
    href="https://speech.microsoft.com/portal/voicegallery"
  >
    Browse available voices and languages
  </Card>
</CardGroup>

## Installation

To use Azure services, install the required dependencies:

```bash
pip install "pipecat-ai[azure]"
```

## Prerequisites

### Azure Account Setup

Before using Azure TTS services, you need:

1. **Azure Account**: Sign up at [Azure Portal](https://portal.azure.com/)
2. **Speech Service**: Create a Speech resource in your Azure subscription
3. **API Key and Region**: Get your subscription key and service region
4. **Voice Selection**: Choose from available voices in the [Voice Gallery](https://speech.microsoft.com/portal/voicegallery)

### Required Environment Variables

- `AZURE_SPEECH_API_KEY`: Your Azure Speech service API key
- `AZURE_SPEECH_REGION`: Your Azure Speech service region (e.g., "eastus")

## Configuration

### AzureTTSService

<ParamField path="api_key" type="str" required>
  Azure Cognitive Services subscription key.
</ParamField>

<ParamField path="region" type="str" required>
  Azure region identifier (e.g., `"eastus"`, `"westus2"`).
</ParamField>

<ParamField path="voice" type="str" default="en-US-SaraNeural">
  Voice name to use for synthesis.
</ParamField>

<ParamField path="sample_rate" type="int" default="None">
  Output audio sample rate in Hz. When `None`, uses the pipeline's configured sample rate.
</ParamField>

<ParamField path="aggregate_sentences" type="bool" default="True">
  Whether to aggregate sentences before synthesis.
</ParamField>

<ParamField path="params" type="InputParams" default="None">
  Runtime-configurable voice and synthesis settings. See [InputParams](#inputparams) below.
</ParamField>

### AzureHttpTTSService

The HTTP service accepts the same parameters as the streaming service except `aggregate_sentences`:

<ParamField path="api_key" type="str" required>
  Azure Cognitive Services subscription key.
</ParamField>

<ParamField path="region" type="str" required>
  Azure region identifier.
</ParamField>

<ParamField path="voice" type="str" default="en-US-SaraNeural">
  Voice name to use for synthesis.
</ParamField>

<ParamField path="sample_rate" type="int" default="None">
  Output audio sample rate in Hz.
</ParamField>

<ParamField path="params" type="InputParams" default="None">
  Voice and synthesis parameters. See [InputParams](#inputparams) below.
</ParamField>

### InputParams

Voice and synthesis settings shared by both service variants. Can be set at initialization via the `params` constructor argument, or changed at runtime via [`UpdateSettingsFrame`](/server/utilities/frame-processor-events#updatesettingsframe).

| Parameter | Type | Default | Description |
| --- | --- | --- | --- |
| `emphasis` | `str` | `None` | Emphasis level for speech (`"strong"`, `"moderate"`, `"reduced"`). |
| `language` | `Language` | `Language.EN_US` | Language for synthesis. |
| `pitch` | `str` | `None` | Voice pitch adjustment (e.g., `"+10%"`, `"-5Hz"`, `"high"`). |
| `rate` | `str` | `None` | Speech rate adjustment (e.g., `"1.0"`, `"1.25"`, `"slow"`, `"fast"`). |
| `role` | `str` | `None` | Voice role for expression (e.g., `"YoungAdultFemale"`). |
| `style` | `str` | `None` | Speaking style (e.g., `"cheerful"`, `"sad"`, `"excited"`). |
| `style_degree` | `str` | `None` | Intensity of the speaking style (0.01 to 2.0). |
| `volume` | `str` | `None` | Volume level (e.g., `"+20%"`, `"loud"`, `"x-soft"`). |

## Usage

### Basic Setup

```python
from pipecat.services.azure import AzureTTSService

tts = AzureTTSService(
    api_key=os.getenv("AZURE_SPEECH_API_KEY"),
    region=os.getenv("AZURE_SPEECH_REGION"),
    voice="en-US-SaraNeural",
)
```

### With Voice Customization

```python
from pipecat.transcriptions.language import Language

tts = AzureTTSService(
    api_key=os.getenv("AZURE_SPEECH_API_KEY"),
    region="eastus",
    voice="en-US-JennyMultilingualNeural",
    params=AzureTTSService.InputParams(
        language=Language.EN_US,
        style="cheerful",
        style_degree="1.5",
        rate="1.1",
    ),
)
```

### HTTP Service

```python
from pipecat.services.azure import AzureHttpTTSService

tts = AzureHttpTTSService(
    api_key=os.getenv("AZURE_SPEECH_API_KEY"),
    region=os.getenv("AZURE_SPEECH_REGION"),
    voice="en-US-SaraNeural",
)
```

## Notes

- **Streaming vs HTTP**: The streaming service (`AzureTTSService`) provides word-level timestamps and lower latency, making it better for interactive conversations. The HTTP service is simpler but returns the complete audio at once.
- **SSML support**: Both services automatically construct SSML from the `InputParams` settings. Special characters in text are automatically escaped.
- **Word timestamps**: `AzureTTSService` supports word-level timestamps for synchronized text display. CJK languages receive special handling to merge individual characters into meaningful word units.
- **8kHz workaround**: At 8kHz sample rates, Azure's reported audio duration may not match word boundary offsets. The service uses word boundary offsets for timing in this case.
