---
title: "AWS Bedrock"
description: "Large Language Model service implementation using Amazon Bedrock API"
---

## Overview

AWS Bedrock LLM service provides access to Amazon's foundation models including Anthropic Claude and Amazon Nova, with streaming responses, function calling, and multimodal capabilities through Amazon's managed AI service.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.aws.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="AWS Bedrock Docs"
    icon="book"
    href="https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html"
  >
    Official AWS Bedrock documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14r-function-calling-aws.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use AWS Bedrock services, install the required dependencies:

```bash
pip install "pipecat-ai[aws]"
```

You'll also need to set up your AWS credentials as environment variables:

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN` (if using temporary credentials)
- `AWS_REGION` (defaults to "us-east-1")

<Tip>
  Set up an IAM user with Amazon Bedrock access in your AWS account to obtain
  credentials.
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/learn/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/learn/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.aws.llm import AWSBedrockLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure the service
llm = AWSBedrockLLMService(
    aws_region="us-west-2",
    model="us.anthropic.claude-3-5-haiku-20241022-v1:0",
    params=AWSBedrockLLMService.InputParams(
        temperature=0.7,
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Register function handler
async def get_current_weather(params):
    location = params.arguments["location"]
    format_type = params.arguments["format"]
    result = {"conditions": "sunny", "temperature": "75", "unit": format_type}
    await params.result_callback(result)

llm.register_function("get_current_weather", get_current_weather)

# Create context with system message
messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant with access to weather information."
    }
]

context = OpenAILLMContext(messages, tools)
context_aggregator = llm.create_context_aggregator(context)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),    # Handles user messages
    llm,                          # Processes with AWS Bedrock
    tts,
    transport.output(),
    context_aggregator.assistant() # Captures responses
])
```

## Metrics

The service provides comprehensive AWS Bedrock metrics:

- **Time to First Byte (TTFB)** - Latency from request to first response token
- **Processing Duration** - Total request processing time
- **Token Usage** - Input tokens, output tokens, and total usage

<Info>
  [Learn how to enable Metrics](/guides/features/metrics) in your Pipeline.
</Info>

## Additional Notes

- **Streaming Responses**: All responses are streamed for low latency
- **Context Persistence**: Use context aggregators to maintain conversation history
- **Error Handling**: Automatic retry logic for rate limits and transient errors
- **Message Format**: Automatically converts between OpenAI and AWS Bedrock message formats
- **Performance Modes**: Choose "standard" or "optimized" latency based on your needs
- **Regional Availability**: Different models available in different AWS regions
- **Vision Support**: Image processing available with compatible models like Claude 3
