---
title: "AWS Bedrock"
description: "Large Language Model service implementation using Amazon Bedrock API"
---

## Overview

`AWSBedrockLLMService` provides access to Amazon's foundation models including Anthropic Claude and Amazon Nova, with streaming responses, function calling, and multimodal capabilities through Amazon's managed AI service for enterprise-grade LLM deployment.

<CardGroup cols={2}>
  <Card
    title="AWS Bedrock LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.aws.llm.html"
  >
    Pipecat's API methods for AWS Bedrock integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14r-function-calling-aws.py"
  >
    Complete example with function calling
  </Card>
  <Card
    title="AWS Bedrock Documentation"
    icon="book"
    href="https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html"
  >
    Official AWS Bedrock documentation and features
  </Card>
  <Card
    title="AWS Console"
    icon="microphone"
    href="https://console.aws.amazon.com/bedrock/"
  >
    Access foundation models and manage IAM
  </Card>
</CardGroup>

## Installation

To use AWS Bedrock services, install the required dependencies:

```bash
pip install "pipecat-ai[aws]"
```

## Prerequisites

### AWS Account Setup

Before using AWS Bedrock LLM services, you need:

1. **AWS Account**: Sign up at [AWS Console](https://console.aws.amazon.com/)
2. **IAM User**: Create an IAM user with Amazon Bedrock permissions
3. **Model Access**: Request access to foundation models in your AWS region
4. **Credentials**: Set up AWS access keys and region configuration

### Required Environment Variables

- `AWS_ACCESS_KEY_ID`: Your AWS access key ID
- `AWS_SECRET_ACCESS_KEY`: Your AWS secret access key
- `AWS_SESSION_TOKEN`: Session token (if using temporary credentials)
- `AWS_REGION`: AWS region (defaults to "us-east-1")

## Configuration

<ParamField path="model" type="str" required>
  AWS Bedrock model identifier (e.g.,
  `"us.anthropic.claude-sonnet-4-5-20250929-v1:0"`,
  `"us.amazon.nova-pro-v1:0"`).
</ParamField>

<ParamField path="aws_access_key" type="str" default="None">
  AWS access key ID. If `None`, uses the `AWS_ACCESS_KEY_ID` environment
  variable or default credential chain.
</ParamField>

<ParamField path="aws_secret_key" type="str" default="None">
  AWS secret access key. If `None`, uses the `AWS_SECRET_ACCESS_KEY` environment
  variable or default credential chain.
</ParamField>

<ParamField path="aws_session_token" type="str" default="None">
  AWS session token for temporary credentials. If `None`, uses the
  `AWS_SESSION_TOKEN` environment variable.
</ParamField>

<ParamField path="aws_region" type="str" default="None">
  AWS region for the Bedrock service. If `None`, uses the `AWS_REGION`
  environment variable, defaulting to `"us-east-1"`.
</ParamField>

<ParamField path="params" type="InputParams" default="None">
  Runtime-configurable model settings. See [InputParams](#inputparams) below.
</ParamField>

<ParamField path="client_config" type="botocore.config.Config" default="None">
  Custom boto3 client configuration. If `None`, uses defaults with 5-minute
  connect/read timeouts and 3 retry attempts.
</ParamField>

<ParamField path="retry_timeout_secs" type="float" default="5.0">
  Request timeout in seconds. Used when `retry_on_timeout` is enabled to
  determine when to retry.
</ParamField>

<ParamField path="retry_on_timeout" type="bool" default="False">
  Whether to retry the request once if it times out. The retry attempt has no
  timeout limit.
</ParamField>

### InputParams

Model inference settings that can be set at initialization via the `params` constructor argument, or changed at runtime via [`UpdateSettingsFrame`](/server/utilities/frame-processor-events#updatesettingsframe).

| Parameter                         | Type        | Default | Description                                                                                        |
| --------------------------------- | ----------- | ------- | -------------------------------------------------------------------------------------------------- |
| `max_tokens`                      | `int`       | `None`  | Maximum number of tokens to generate. Must be at least 1.                                          |
| `temperature`                     | `float`     | `None`  | Sampling temperature (0.0 to 1.0). Lower values are more focused, higher values are more creative. |
| `top_p`                           | `float`     | `None`  | Top-p (nucleus) sampling (0.0 to 1.0). Controls diversity of output.                               |
| `stop_sequences`                  | `List[str]` | `[]`    | List of strings that stop generation when encountered.                                             |
| `latency`                         | `str`       | `None`  | Performance mode: `"standard"` or `"optimized"`.                                                   |
| `additional_model_request_fields` | `dict`      | `{}`    | Additional model-specific parameters passed directly to the API.                                   |

<Note>
  `None` values are omitted from the inference config, letting the Bedrock API
  use its own defaults. Only parameters that are explicitly set are included in
  the request. This avoids conflicts with models that don't allow certain
  parameter combinations (e.g., `temperature` and `top_p` together).
</Note>

## Usage

### Basic Setup

```python
from pipecat.services.aws import AWSBedrockLLMService

llm = AWSBedrockLLMService(
    model="us.anthropic.claude-sonnet-4-5-20250929-v1:0",
    aws_access_key=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    aws_region=os.getenv("AWS_REGION", "us-east-1"),
)
```

### With Custom Parameters

```python
from pipecat.services.aws import AWSBedrockLLMService

llm = AWSBedrockLLMService(
    model="us.amazon.nova-pro-v1:0",
    aws_access_key=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    aws_region="us-east-1",
    params=AWSBedrockLLMService.InputParams(
        max_tokens=2048,
        temperature=0.7,
        latency="optimized",
    ),
)
```

### Updating Settings at Runtime

Model settings can be changed mid-conversation using `UpdateSettingsFrame`:

```python
from pipecat.frames.frames import UpdateSettingsFrame

await task.queue_frame(
    UpdateSettingsFrame(
        settings={
            "llm": {
                "temperature": 0.3,
                "max_tokens": 1024,
            }
        }
    )
)
```

## Notes

- **Credential chain**: If `aws_access_key` and `aws_secret_key` are not provided, the service falls back to environment variables and then the standard AWS credential chain (IAM roles, instance profiles, etc.).
- **No-op tool handling**: AWS Bedrock requires at least one tool to be defined when tool content exists in the conversation. The service automatically adds a placeholder tool when needed to prevent API errors.
- **Model-specific parameters**: Some models (e.g., Claude Sonnet 4.5) don't allow certain parameter combinations. The service only includes explicitly set parameters in the inference config to avoid conflicts.
- **Retry behavior**: When `retry_on_timeout=True`, the first attempt uses the `retry_timeout_secs` timeout. If it times out, a second attempt is made with no timeout limit.

## Event Handlers

`AWSBedrockLLMService` supports the following event handlers, inherited from [LLMService](/server/utilities/service-events):

| Event                       | Description                                                             |
| --------------------------- | ----------------------------------------------------------------------- |
| `on_completion_timeout`     | Called when an LLM completion request times out                         |
| `on_function_calls_started` | Called when function calls are received and execution is about to start |

```python
@llm.event_handler("on_completion_timeout")
async def on_completion_timeout(service):
    print("LLM completion timed out")
```
