---
title: "Grok"
description: "LLM service implementation using Grok's API with OpenAI-compatible interface"
---

## Overview

`GrokLLMService` provides access to Grok's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management with Grok's unique reasoning capabilities.

<CardGroup cols={2}>
  <Card
    title="Grok LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.grok.llm.html"
  >
    Pipecat's API methods for Grok integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14g-function-calling-grok.py"
  >
    Complete example with function calling
  </Card>
  <Card
    title="Grok Documentation"
    icon="book"
    href="https://docs.x.ai/docs/api-reference#chat-completions"
  >
    Official Grok API documentation and features
  </Card>
  <Card title="X.AI Platform" icon="microphone" href="https://console.x.ai/">
    Access Grok models and manage API keys
  </Card>
</CardGroup>

## Installation

To use Grok services, install the required dependencies:

```bash
pip install "pipecat-ai[grok]"
```

## Prerequisites

### Grok Account Setup

Before using Grok LLM services, you need:

1. **X.AI Account**: Sign up at [X.AI Console](https://console.x.ai/)
2. **API Key**: Generate an API key from your console dashboard
3. **Model Selection**: Choose from available Grok models

### Required Environment Variables

- `XAI_API_KEY`: Your X.AI API key for authentication
