---
title: "OpenPipe"
description: "LLM service implementation using OpenPipe for LLM request logging and fine-tuning"
---

## Overview

`OpenPipeLLMService` extends the BaseOpenAILLMService to provide integration with OpenPipe, enabling request logging, model fine-tuning, and performance monitoring. It maintains compatibility with OpenAI's API while adding OpenPipe's logging and optimization capabilities.

<CardGroup cols={2}>
  <Card
    title="OpenPipe LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openpipe.llm.html"
  >
    Pipecat's API methods for OpenPipe integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/tree/main/examples"
  >
    Browse examples using OpenPipe logging
  </Card>
  <Card
    title="OpenPipe Documentation"
    icon="book"
    href="https://docs.openpipe.ai/api-reference/post-chatcompletions"
  >
    Official OpenPipe API documentation and features
  </Card>
  <Card title="OpenPipe Platform" icon="microphone" href="https://openpipe.ai/">
    Access logging and fine-tuning features
  </Card>
</CardGroup>

## Installation

To use OpenPipe services, install the required dependencies:

```bash
pip install "pipecat-ai[openpipe]"
```

## Prerequisites

### OpenPipe Account Setup

Before using OpenPipe LLM services, you need:

1. **OpenPipe Account**: Sign up at [OpenPipe](https://openpipe.ai/)
2. **API Keys**: Generate both OpenPipe and OpenAI API keys
3. **Project Setup**: Configure logging and fine-tuning projects

### Required Environment Variables

- `OPENPIPE_API_KEY`: Your OpenPipe API key for logging and fine-tuning
- `OPENAI_API_KEY`: Your OpenAI API key for underlying model access

## Configuration

<ParamField path="model" type="str" default="gpt-4.1">
  The model name to use for completions.
</ParamField>

<ParamField path="api_key" type="str" default="None">
  OpenAI API key for authentication. If not provided, reads from environment.
</ParamField>

<ParamField path="base_url" type="str" default="None">
  Custom OpenAI API endpoint URL. Uses the default OpenAI URL if not provided.
</ParamField>

<ParamField path="openpipe_api_key" type="str" default="None">
  OpenPipe API key for request logging and fine-tuning features. If not
  provided, reads from environment.
</ParamField>

<ParamField
  path="openpipe_base_url"
  type="str"
  default="https://app.openpipe.ai/api/v1"
>
  OpenPipe API endpoint URL.
</ParamField>

<ParamField path="tags" type="Dict[str, str]" default="None">
  Dictionary of tags to apply to all requests for tracking and filtering in the
  OpenPipe dashboard.
</ParamField>

### InputParams

This service uses the same input parameters as `OpenAILLMService`. See [OpenAI LLM](/server/services/llm/openai#inputparams) for details.

## Usage

### Basic Setup

```python
import os
from pipecat.services.openpipe import OpenPipeLLMService

llm = OpenPipeLLMService(
    api_key=os.getenv("OPENAI_API_KEY"),
    openpipe_api_key=os.getenv("OPENPIPE_API_KEY"),
    model="gpt-4.1",
)
```

### With Tags for Tracking

```python
from pipecat.services.openpipe import OpenPipeLLMService

llm = OpenPipeLLMService(
    api_key=os.getenv("OPENAI_API_KEY"),
    openpipe_api_key=os.getenv("OPENPIPE_API_KEY"),
    model="gpt-4.1",
    tags={
        "environment": "production",
        "project": "voice-assistant",
    },
)
```

## Notes

- All requests are automatically logged to OpenPipe for monitoring and fine-tuning purposes.
- Tags are included with every request and can be used to filter and organize requests in the OpenPipe dashboard.
- OpenPipe uses its own client (`openpipe.AsyncOpenAI`) instead of the standard OpenAI client to enable transparent request logging.
