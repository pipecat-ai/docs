---
title: "Groq"
description: "LLM service implementation using Groq's API with OpenAI-compatible interface"
---

## Overview

`GroqLLMService` provides access to Groq's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management with ultra-fast inference speeds.

<CardGroup cols={2}>
  <Card
    title="Groq LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.groq.llm.html"
  >
    Pipecat's API methods for Groq integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14f-function-calling-groq.py"
  >
    Complete example with function calling
  </Card>
  <Card
    title="Groq Documentation"
    icon="book"
    href="https://console.groq.com/docs/api-reference#chat-create"
  >
    Official Groq API documentation and features
  </Card>
  <Card title="Groq Console" icon="microphone" href="https://console.groq.com/">
    Access models and manage API keys
  </Card>
</CardGroup>

## Installation

To use Groq services, install the required dependency:

```bash
pip install "pipecat-ai[groq]"
```

## Prerequisites

### Groq Account Setup

Before using Groq LLM services, you need:

1. **Groq Account**: Sign up at [Groq Console](https://console.groq.com/)
2. **API Key**: Generate an API key from your console dashboard
3. **Model Selection**: Choose from available models with ultra-fast inference

### Required Environment Variables

- `GROQ_API_KEY`: Your Groq API key for authentication

## Configuration

<ParamField path="api_key" type="str" required>
  Groq API key for authentication.
</ParamField>

<ParamField path="base_url" type="str" default="https://api.groq.com/openai/v1">
  Base URL for Groq API endpoint.
</ParamField>

<ParamField path="model" type="str" default="llama-3.3-70b-versatile">
  Model identifier to use.
</ParamField>

### InputParams

This service uses the same input parameters as `OpenAILLMService`. See [OpenAI LLM](/server/services/llm/openai#inputparams) for details.

## Usage

### Basic Setup

```python
import os
from pipecat.services.groq import GroqLLMService

llm = GroqLLMService(
    api_key=os.getenv("GROQ_API_KEY"),
    model="llama-3.3-70b-versatile",
)
```

### With Custom Parameters

```python
from pipecat.services.groq import GroqLLMService

llm = GroqLLMService(
    api_key=os.getenv("GROQ_API_KEY"),
    model="llama-3.3-70b-versatile",
    params=GroqLLMService.InputParams(
        temperature=0.7,
        top_p=0.9,
        max_completion_tokens=1024,
    ),
)
```

## Notes

- Groq provides ultra-fast inference using custom LPU (Language Processing Unit) hardware.
- Groq fully supports the OpenAI-compatible parameter set inherited from `OpenAILLMService`.
