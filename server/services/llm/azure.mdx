---
title: "Azure"
description: "Large Language Model service implementation using Azure OpenAI API"
---

## Overview

`AzureLLMService` provides access to Azure OpenAI's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management with enterprise-grade security and compliance.

<CardGroup cols={2}>
  <Card
    title="Azure LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.azure.llm.html"
  >
    Pipecat's API methods for Azure OpenAI integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14h-function-calling-azure.py"
  >
    Complete example with function calling
  </Card>
  <Card
    title="Azure OpenAI Documentation"
    icon="book"
    href="https://learn.microsoft.com/en-us/azure/ai-services/openai/"
  >
    Official Azure OpenAI documentation and setup
  </Card>
  <Card title="Azure Portal" icon="microphone" href="https://portal.azure.com/">
    Create OpenAI resources and get credentials
  </Card>
</CardGroup>

## Installation

To use Azure OpenAI services, install the required dependency:

```bash
pip install "pipecat-ai[azure]"
```

## Prerequisites

### Azure OpenAI Setup

Before using Azure OpenAI LLM services, you need:

1. **Azure Account**: Sign up at [Azure Portal](https://portal.azure.com/)
2. **OpenAI Resource**: Create an Azure OpenAI resource in your subscription
3. **Model Deployment**: Deploy your chosen model (GPT-4, GPT-4o, etc.)
4. **Credentials**: Get your API key, endpoint, and deployment name

### Required Environment Variables

- `AZURE_CHATGPT_API_KEY`: Your Azure OpenAI API key
- `AZURE_CHATGPT_ENDPOINT`: Your Azure OpenAI endpoint URL
- `AZURE_CHATGPT_MODEL`: Your model deployment name

## Configuration

<ParamField path="api_key" type="str" required>
  Azure OpenAI API key for authentication.
</ParamField>

<ParamField path="endpoint" type="str" required>
  Azure OpenAI endpoint URL (e.g., `"https://your-resource.openai.azure.com/"`).
</ParamField>

<ParamField path="model" type="str" required>
  Azure model deployment name. This is the name you gave when deploying the
  model, not the base model name.
</ParamField>

<ParamField path="api_version" type="str" default="2024-09-01-preview">
  Azure OpenAI API version string.
</ParamField>

Since `AzureLLMService` inherits from `OpenAILLMService`, it also accepts the following parameters:

<ParamField path="params" type="InputParams" default="None">
  Runtime-configurable model settings. See [OpenAI
  InputParams](/server/services/llm/openai#inputparams) for details.
</ParamField>

<ParamField path="retry_timeout_secs" type="float" default="5.0">
  Request timeout in seconds. Used when `retry_on_timeout` is enabled to
  determine when to retry.
</ParamField>

<ParamField path="retry_on_timeout" type="bool" default="False">
  Whether to retry the request once if it times out. The retry attempt has no
  timeout limit.
</ParamField>

### InputParams

`AzureLLMService` uses the same `InputParams` as `OpenAILLMService`. See the [OpenAI InputParams](/server/services/llm/openai#inputparams) section for the full parameter reference.

## Usage

### Basic Setup

```python
from pipecat.services.azure import AzureLLMService

llm = AzureLLMService(
    api_key=os.getenv("AZURE_CHATGPT_API_KEY"),
    endpoint=os.getenv("AZURE_CHATGPT_ENDPOINT"),
    model=os.getenv("AZURE_CHATGPT_MODEL"),
)
```

### With Custom Parameters

```python
from pipecat.services.azure import AzureLLMService

llm = AzureLLMService(
    api_key=os.getenv("AZURE_CHATGPT_API_KEY"),
    endpoint=os.getenv("AZURE_CHATGPT_ENDPOINT"),
    model=os.getenv("AZURE_CHATGPT_MODEL"),
    api_version="2024-09-01-preview",
    params=AzureLLMService.InputParams(
        temperature=0.7,
        max_completion_tokens=1000,
        frequency_penalty=0.5,
    ),
)
```

### Updating Settings at Runtime

Model settings can be changed mid-conversation using `UpdateSettingsFrame`:

```python
from pipecat.frames.frames import UpdateSettingsFrame

await task.queue_frame(
    UpdateSettingsFrame(
        settings={
            "llm": {
                "temperature": 0.3,
                "max_completion_tokens": 500,
            }
        }
    )
)
```

## Notes

- **Deployment name vs model name**: The `model` parameter should be your Azure deployment name, not the underlying model name (e.g., use `"my-gpt4-deployment"` instead of `"gpt-4"`).
- **API version**: Different API versions support different features. Check the [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference) for version-specific capabilities.
- **Full OpenAI compatibility**: Since `AzureLLMService` inherits from `OpenAILLMService`, it supports all the same features including function calling, vision input, and streaming responses.

## Event Handlers

`AzureLLMService` supports the same event handlers as `OpenAILLMService`, inherited from [LLMService](/server/utilities/service-events):

| Event                       | Description                                                             |
| --------------------------- | ----------------------------------------------------------------------- |
| `on_completion_timeout`     | Called when an LLM completion request times out                         |
| `on_function_calls_started` | Called when function calls are received and execution is about to start |

```python
@llm.event_handler("on_completion_timeout")
async def on_completion_timeout(service):
    print("LLM completion timed out")
```
