---
title: "Ollama"
description: "LLM service implementation using Ollama with OpenAI-compatible interface"
---

## Overview

`OLLamaLLMService` provides access to locally-run Ollama models through an OpenAI-compatible interface. It inherits from `BaseOpenAILLMService` and allows you to run various open-source models locally while maintaining compatibility with OpenAI's API format for privacy and cost control.

<CardGroup cols={2}>
  <Card
    title="Ollama LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.ollama.llm.html"
  >
    Pipecat's API methods for Ollama integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/tree/main/examples"
  >
    Browse examples using Ollama models
  </Card>
  <Card
    title="Ollama Documentation"
    icon="book"
    href="https://ollama.com/library"
  >
    Official Ollama documentation and model library
  </Card>
  <Card
    title="Download Ollama"
    icon="microphone"
    href="https://ollama.com/download"
  >
    Download and setup instructions for Ollama
  </Card>
</CardGroup>

## Installation

To use Ollama services, you need to install both Ollama and the Pipecat dependency:

1. **Install Ollama** on your system from [ollama.com/download](https://ollama.com/download)
2. **Install Pipecat dependency**:

```bash
pip install "pipecat-ai[ollama]"
```

3. **Pull a model** (first time only):

```bash
ollama pull llama2
```

## Prerequisites

### Ollama Local Setup

Before using Ollama LLM services, you need:

1. **Ollama Installation**: Download and install Ollama from [ollama.com](https://ollama.com/download)
2. **Model Selection**: Pull your desired models (llama2, mistral, codellama, etc.)
3. **Local Service**: Ensure Ollama service is running (default port 11434)
4. **Hardware**: Sufficient RAM and storage for your chosen models

### Configuration

- **No API Keys Required**: Ollama runs entirely locally
- **Model Management**: Use `ollama pull <model>` to download models
- **Service URL**: Default is `http://localhost:11434` (configurable)

<Tip>
  Ollama runs as a local service on port 11434. No API key required for complete
  privacy!
</Tip>
