---
title: "OpenRouter"
description: "LLM service implementation using OpenRouter's API with OpenAI-compatible interface"
---

## Overview

`OpenRouterLLMService` provides access to OpenRouter's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management with access to multiple model providers through a single API.

<CardGroup cols={2}>
  <Card
    title="OpenRouter LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openrouter.llm.html"
  >
    Pipecat's API methods for OpenRouter integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14m-function-calling-openrouter.py"
  >
    Complete example with function calling
  </Card>
  <Card
    title="OpenRouter Documentation"
    icon="book"
    href="https://openrouter.ai/docs/api-reference/chat-completion"
  >
    Official OpenRouter API documentation and features
  </Card>
  <Card
    title="OpenRouter Platform"
    icon="microphone"
    href="https://openrouter.ai/"
  >
    Access multiple model providers and manage API keys
  </Card>
</CardGroup>

## Installation

To use OpenRouter services, install the required dependencies:

```bash
pip install "pipecat-ai[openrouter]"
```

## Prerequisites

### OpenRouter Account Setup

Before using OpenRouter LLM services, you need:

1. **OpenRouter Account**: Sign up at [OpenRouter](https://openrouter.ai/)
2. **API Key**: Generate an API key from your account dashboard
3. **Model Selection**: Choose from hundreds of available models from different providers
4. **Credits**: Add credits to your account for model usage

### Required Environment Variables

- `OPENROUTER_API_KEY`: Your OpenRouter API key for authentication

## Configuration

<ParamField path="api_key" type="str" default="None">
  OpenRouter API key for authentication. If not provided, the client will
  attempt to read from environment variables.
</ParamField>

<ParamField path="model" type="str" default="openai/gpt-4o-2024-11-20">
  Model identifier to use. Uses the `provider/model` format.
</ParamField>

<ParamField path="base_url" type="str" default="https://openrouter.ai/api/v1">
  Base URL for OpenRouter API endpoint.
</ParamField>

### InputParams

This service uses the same input parameters as `OpenAILLMService`. See [OpenAI LLM](/server/services/llm/openai#inputparams) for details.

## Usage

### Basic Setup

```python
import os
from pipecat.services.openrouter import OpenRouterLLMService

llm = OpenRouterLLMService(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    model="openai/gpt-4o-2024-11-20",
)
```

### With a Different Provider Model

```python
from pipecat.services.openrouter import OpenRouterLLMService

llm = OpenRouterLLMService(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    model="anthropic/claude-sonnet-4-20250514",
    params=OpenRouterLLMService.InputParams(
        temperature=0.7,
        max_completion_tokens=1024,
    ),
)
```

## Notes

- OpenRouter model identifiers use the `provider/model` format (e.g., `openai/gpt-4o`, `anthropic/claude-sonnet-4-20250514`, `google/gemini-pro`).
- When using Gemini models through OpenRouter, the service automatically handles the constraint that only one system message is allowed by converting additional system messages to user messages.
