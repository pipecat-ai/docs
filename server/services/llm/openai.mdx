---
title: "OpenAI"
description: "Large Language Model services using OpenAI's chat completion API"
---

## Overview

`OpenAILLMService` provides chat completion capabilities using OpenAI's API, supporting streaming responses, function calling, vision input, and advanced context management for conversational AI applications with state-of-the-art language models.

<CardGroup cols={2}>
  <Card
    title="OpenAI LLM API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openai.base_llm.html"
  >
    Pipecat's API methods for OpenAI integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14-function-calling.py"
  >
    Function calling example with weather API
  </Card>
  <Card
    title="OpenAI Documentation"
    icon="book"
    href="https://platform.openai.com/docs/api-reference/chat"
  >
    Official OpenAI API documentation
  </Card>
  <Card
    title="OpenAI Platform"
    icon="microphone"
    href="https://platform.openai.com/api-keys"
  >
    Access models and manage API keys
  </Card>
</CardGroup>

## Installation

To use OpenAI services, install the required dependencies:

```bash
pip install "pipecat-ai[openai]"
```

## Prerequisites

### OpenAI Account Setup

Before using OpenAI LLM services, you need:

1. **OpenAI Account**: Sign up at [OpenAI Platform](https://platform.openai.com/)
2. **API Key**: Generate an API key from your account dashboard
3. **Model Selection**: Choose from available models (GPT-4.1, GPT-4o, GPT-4o-mini, etc.)
4. **Usage Limits**: Set up billing and usage limits as needed

### Required Environment Variables

- `OPENAI_API_KEY`: Your OpenAI API key for authentication

## Configuration

<ParamField path="model" type="str" default="gpt-4.1">
  OpenAI model name to use (e.g., `"gpt-4.1"`, `"gpt-4o"`, `"gpt-4o-mini"`).
</ParamField>

<ParamField path="api_key" type="str" default="None">
  OpenAI API key. If `None`, uses the `OPENAI_API_KEY` environment variable.
</ParamField>

<ParamField path="base_url" type="str" default="None">
  Custom base URL for the OpenAI API. Override for proxied or self-hosted deployments.
</ParamField>

<ParamField path="organization" type="str" default="None">
  OpenAI organization ID.
</ParamField>

<ParamField path="project" type="str" default="None">
  OpenAI project ID.
</ParamField>

<ParamField path="default_headers" type="Mapping[str, str]" default="None">
  Additional HTTP headers to include in every request.
</ParamField>

<ParamField path="params" type="InputParams" default="None">
  Runtime-configurable model settings. See [InputParams](#inputparams) below.
</ParamField>

<ParamField path="retry_timeout_secs" type="float" default="5.0">
  Request timeout in seconds. Used when `retry_on_timeout` is enabled to determine when to retry.
</ParamField>

<ParamField path="retry_on_timeout" type="bool" default="False">
  Whether to retry the request once if it times out. The retry attempt has no timeout limit.
</ParamField>

### InputParams

Model inference settings that can be set at initialization via the `params` constructor argument, or changed at runtime via [`UpdateSettingsFrame`](/server/utilities/frame-processor-events#updatesettingsframe).

| Parameter | Type | Default | Description |
| --- | --- | --- | --- |
| `frequency_penalty` | `float` | `NOT_GIVEN` | Penalty for frequent tokens (-2.0 to 2.0). Positive values discourage repetition. |
| `presence_penalty` | `float` | `NOT_GIVEN` | Penalty for new topics (-2.0 to 2.0). Positive values encourage the model to talk about new topics. |
| `seed` | `int` | `NOT_GIVEN` | Random seed for deterministic outputs. |
| `temperature` | `float` | `NOT_GIVEN` | Sampling temperature (0.0 to 2.0). Lower values are more focused, higher values are more creative. |
| `top_k` | `int` | `None` | Top-k sampling parameter. Currently ignored by the OpenAI client library. |
| `top_p` | `float` | `NOT_GIVEN` | Top-p (nucleus) sampling (0.0 to 1.0). Controls diversity of output. |
| `max_tokens` | `int` | `NOT_GIVEN` | Maximum tokens in response. Deprecated -- use `max_completion_tokens` instead. |
| `max_completion_tokens` | `int` | `NOT_GIVEN` | Maximum completion tokens to generate. |
| `service_tier` | `str` | `NOT_GIVEN` | Service tier (e.g., `"auto"`, `"flex"`, `"priority"`). Controls latency and cost tradeoffs. |
| `extra` | `dict` | `{}` | Additional model-specific parameters passed directly to the API. |

<Note>
  `NOT_GIVEN` values are omitted from the API request entirely, letting the OpenAI API use its own defaults. This is different from `None`, which would be sent explicitly.
</Note>

## Usage

### Basic Setup

```python
from pipecat.services.openai import OpenAILLMService

llm = OpenAILLMService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o",
)
```

### With Custom Parameters

```python
from pipecat.services.openai import OpenAILLMService

llm = OpenAILLMService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4.1",
    params=OpenAILLMService.InputParams(
        temperature=0.7,
        max_completion_tokens=1000,
        frequency_penalty=0.5,
    ),
)
```

### Updating Settings at Runtime

Model settings can be changed mid-conversation using `UpdateSettingsFrame`:

```python
from pipecat.frames.frames import UpdateSettingsFrame

await task.queue_frame(
    UpdateSettingsFrame(
        settings={
            "llm": {
                "temperature": 0.3,
                "max_completion_tokens": 500,
            }
        }
    )
)
```

## Notes

- **OpenAI-compatible providers**: Many third-party LLM providers offer OpenAI-compatible APIs. You can use `OpenAILLMService` with them by setting `base_url` to the provider's endpoint.
- **Retry behavior**: When `retry_on_timeout=True`, the first attempt uses the `retry_timeout_secs` timeout. If it times out, a second attempt is made with no timeout limit.
- **Function calling**: Supports OpenAI's tool/function calling format. Register function handlers on the pipeline task to handle tool calls automatically.

## Event Handlers

`OpenAILLMService` supports the following event handlers, inherited from [LLMService](/server/utilities/service-events):

| Event | Description |
| --- | --- |
| `on_completion_timeout` | Called when an LLM completion request times out |
| `on_function_calls_started` | Called when function calls are received and execution is about to start |

```python
@llm.event_handler("on_completion_timeout")
async def on_completion_timeout(service):
    print("LLM completion timed out")
```
