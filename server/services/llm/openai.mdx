---
title: "OpenAI"
description: "Large Language Model services using OpenAI’s chat completion API"
---

## Overview

`OpenAILLMService` provides chat completion capabilities using OpenAI's API, supporting streaming responses, function calling, vision input, and advanced context management for conversational AI applications.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openai.base_llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="OpenAI Docs"
    icon="book"
    href="https://platform.openai.com/docs/api-reference/chat"
  >
    Official OpenAI API documentation
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14-function-calling.py"
  >
    Function calling example with weather API
  </Card>
</CardGroup>

## Installation

To use OpenAI services, install the required dependencies:

```bash
pip install "pipecat-ai[openai]"
```

You'll also need to set up your OpenAI API key as an environment variable: `OPENAI_API_KEY`.

<Tip>
  Get your API key from the [OpenAI
  Platform](https://platform.openai.com/api-keys).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - OpenAI-specific conversation context
- `LLMMessagesFrame` - Standard conversation messages
- `VisionImageRawFrame` - Images for vision model processing
- `LLMUpdateSettingsFrame` - Runtime model configuration updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/guides/fundamentals/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/guides/fundamentals/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

### Basic Conversation with Function Calling

```python
import os
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.services.llm_service import FunctionCallParams

# Configure the service
llm = OpenAILLMService(
    model="gpt-4o",
    api_key=os.getenv("OPENAI_API_KEY"),
    params=OpenAILLMService.InputParams(
        temperature=0.7,
    )
)

# Define function schema
weather_function = FunctionSchema(
    name="get_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City name"
        }
    },
    required=["location"]
)

# Create tools and context
tools = ToolsSchema(standard_tools=[weather_function])
context = OpenAILLMContext(
    messages=[{
        "role": "system",
        "content": "You are a helpful assistant. Keep responses concise."
    }],
    tools=tools
)

# Register function handler
async def get_weather_handler(params: FunctionCallParams):
    location = params.arguments.get("location")
    # Call weather API here...
    weather_data = {"temperature": "75°F", "conditions": "sunny"}
    await params.result_callback(weather_data)

llm.register_function("get_weather", get_weather_handler)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),    # Handles user messages
    llm,                          # Processes with OpenAI
    tts,
    transport.output(),
    context_aggregator.assistant() # Captures responses
])
```

## Metrics

The service provides:

- **Time to First Byte (TTFB)** - Latency from request to first response token
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and total usage

Enable with:

```python
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        enable_metrics=True,
        enable_usage_metrics=True
    )
)
```

## Additional Notes

- **Streaming Responses**: All responses are streamed for low latency
- **Context Persistence**: Use context aggregators to maintain conversation history
- **Error Handling**: Automatic retry logic for rate limits and transient errors
- **Compatible Services**: Works with OpenAI-compatible APIs by setting `base_url`
