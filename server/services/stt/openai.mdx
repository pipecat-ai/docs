---
title: "OpenAI"
description: "Speech-to-text service implementations using OpenAI's Speech-to-Text APIs"
---

## Overview

OpenAI provides two STT service implementations:

- `OpenAISTTService` for VAD-segmented speech recognition using OpenAI's transcription API (HTTP-based), supporting GPT-4o transcription and Whisper models
- `OpenAIRealtimeSTTService` for real-time streaming speech-to-text using OpenAI's Realtime API WebSocket transcription sessions, with support for local VAD and server-side VAD modes

<CardGroup cols={2}>
  <Card
    title="OpenAI STT API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.openai.stt.html"
  >
    Pipecat's API methods for OpenAI STT integration
  </Card>
  <Card
    title="Example Implementation"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07g-interruptible-openai.py"
  >
    Complete example with OpenAI ecosystem integration
  </Card>
  <Card
    title="OpenAI Documentation"
    icon="book"
    href="https://platform.openai.com/docs/api-reference/audio/createTranscription"
  >
    Official OpenAI transcription documentation and features
  </Card>
  <Card
    title="OpenAI Platform"
    icon="microphone"
    href="https://platform.openai.com/api-keys"
  >
    Access API keys and transcription models
  </Card>
</CardGroup>

## Installation

To use OpenAI services, install the required dependency:

```bash
pip install "pipecat-ai[openai]"
```

## Prerequisites

### OpenAI Account Setup

Before using OpenAI STT services, you need:

1. **OpenAI Account**: Sign up at [OpenAI Platform](https://platform.openai.com/)
2. **API Key**: Generate an API key from your account dashboard
3. **Model Access**: Ensure access to Whisper and GPT-4o transcription models

### Required Environment Variables

- `OPENAI_API_KEY`: Your OpenAI API key for authentication

## Configuration

### OpenAISTTService

Uses VAD-based audio segmentation with HTTP transcription requests. It records speech segments detected by local VAD and sends them to OpenAI's transcription API.

<ParamField path="model" type="str" default="gpt-4o-transcribe">
  Transcription model to use. Options include `"gpt-4o-transcribe"`,
  `"gpt-4o-mini-transcribe"`, and `"whisper-1"`.
</ParamField>

<ParamField path="api_key" type="str" default="None">
  OpenAI API key. Falls back to the `OPENAI_API_KEY` environment variable.
</ParamField>

<ParamField path="base_url" type="str" default="None">
  API base URL. Override for custom or proxied deployments.
</ParamField>

<ParamField path="language" type="Language" default="Language.EN">
  Language of the audio input.
</ParamField>

<ParamField path="prompt" type="str" default="None">
  Optional text to guide the model's style or continue a previous segment.
</ParamField>

<ParamField path="temperature" type="float" default="None">
  Sampling temperature between 0 and 1. Lower values produce more deterministic
  results.
</ParamField>

### OpenAIRealtimeSTTService

Provides real-time streaming speech-to-text using OpenAI's Realtime API WebSocket transcription sessions. Audio is streamed continuously over a WebSocket connection for lower latency compared to HTTP-based transcription.

<ParamField path="api_key" type="str" required>
  OpenAI API key for authentication.
</ParamField>

<ParamField path="model" type="str" default="gpt-4o-transcribe">
  Transcription model. Supported values are `"gpt-4o-transcribe"` and
  `"gpt-4o-mini-transcribe"`.
</ParamField>

<ParamField
  path="base_url"
  type="str"
  default="wss://api.openai.com/v1/realtime"
>
  WebSocket base URL for the Realtime API.
</ParamField>

<ParamField path="language" type="Language" default="Language.EN">
  Language of the audio input.
</ParamField>

<ParamField path="prompt" type="str" default="None">
  Optional prompt text to guide transcription style or provide keyword hints.
</ParamField>

<ParamField path="turn_detection" type="dict | Literal[False]" default="False">
  Server-side VAD configuration. Defaults to `False` (disabled), which relies on a local VAD processor in the pipeline. Pass `None` to use server defaults (`server_vad`), or a dict with custom settings (e.g. `{"type": "server_vad", "threshold": 0.5}`).
</ParamField>

<ParamField path="noise_reduction" type="str" default="None">
  Noise reduction mode. `"near_field"` for close microphones, `"far_field"` for
  distant microphones, or `None` to disable.
</ParamField>

<ParamField path="should_interrupt" type="bool" default="True">
  Whether to interrupt bot output when speech is detected by server-side VAD.
  Only applies when turn detection is enabled.
</ParamField>

## Usage

### OpenAISTTService

```python
from pipecat.services.openai.stt import OpenAISTTService

stt = OpenAISTTService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o-transcribe",
)
```

### OpenAIRealtimeSTTService with Local VAD

```python
from pipecat.services.openai.stt import OpenAIRealtimeSTTService

# Local VAD mode (default) - use with a VAD processor in the pipeline
stt = OpenAIRealtimeSTTService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o-transcribe",
    noise_reduction="near_field",
)
```

### OpenAIRealtimeSTTService with Server-Side VAD

```python
from pipecat.services.openai.stt import OpenAIRealtimeSTTService

# Server-side VAD mode - do NOT use a separate VAD processor
stt = OpenAIRealtimeSTTService(
    api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o-transcribe",
    turn_detection=None,  # Enable server-side VAD
)
```

## Notes

- **Local VAD vs Server-side VAD**: `OpenAIRealtimeSTTService` defaults to local VAD mode (`turn_detection=False`), where a local VAD processor in the pipeline controls when audio is committed for transcription. Set `turn_detection=None` for server-side VAD, but do not use a separate VAD processor in the pipeline in that mode.
- **Automatic resampling**: `OpenAIRealtimeSTTService` automatically resamples audio to 24 kHz as required by the Realtime API, regardless of the pipeline's sample rate.
- **Segmented vs streaming**: `OpenAISTTService` processes complete audio segments (after VAD detects silence) via HTTP. `OpenAIRealtimeSTTService` streams audio continuously over WebSocket for lower latency.
- **Interim transcriptions**: `OpenAIRealtimeSTTService` produces interim transcriptions via delta events, while `OpenAISTTService` only produces final transcriptions.

## Event Handlers

`OpenAIRealtimeSTTService` supports the standard [service connection events](/server/utilities/service-events):

| Event             | Description                                 |
| ----------------- | ------------------------------------------- |
| `on_connected`    | Connected to OpenAI Realtime WebSocket      |
| `on_disconnected` | Disconnected from OpenAI Realtime WebSocket |

```python
@stt.event_handler("on_connected")
async def on_connected(service):
    print("Connected to OpenAI Realtime STT")
```

<Note>
  `OpenAISTTService` uses HTTP requests and does not have WebSocket connection
  events.
</Note>
