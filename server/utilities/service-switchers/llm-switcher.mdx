---
title: "LLMSwitcher"
description: "Dynamically switch between different LLM services with support for ad-hoc inference and unified function registration"
---

## Overview

`LLMSwitcher` is a specialized version of `ServiceSwitcher` designed specifically for managing multiple LLM services. Beyond basic service switching, it provides convenient methods for running ad-hoc inferences and registering function handlers across all LLMs simultaneously.

This is particularly useful when you need to switch between different LLM providers based on task complexity, cost optimization, or specific model capabilities while maintaining a consistent function calling interface.

## Constructor

```python
from pipecat.pipeline.llm_switcher import LLMSwitcher
from pipecat.pipeline.service_switcher import ServiceSwitcherStrategyManual

switcher = LLMSwitcher(
    llms=[openai_service, gemini_service],
    strategy_type=ServiceSwitcherStrategyManual
)
```

<ParamField path="llms" type="List[LLMService]" required>
  List of LLM service instances to switch between.
</ParamField>

<ParamField path="strategy_type" type="Type[ServiceSwitcherStrategy]" required>
  The strategy class to use for switching logic. Pass the class itself, not an
  instance.
</ParamField>

## Properties

<ParamField path="llms" type="List[LLMService]">
  The list of LLM services managed by this switcher.
</ParamField>

<ParamField path="active_llm" type="Optional[LLMService]">
  The currently active LLM service, or None if no LLMs are configured.
</ParamField>

## Methods

### run_inference()

Run a one-shot inference with the currently active LLM, outside of the normal pipeline flow.

```python
result = await llm_switcher.run_inference(context=llm_context)
```

<ParamField path="context" type="LLMContext" required>
  The LLM context containing conversation history and messages.
</ParamField>

**Returns**: `Optional[str]` - The LLM's response as a string, or None if no LLM is active.

### register_function()

Register a function handler with all LLMs in the switcher, regardless of which is currently active.

```python
llm_switcher.register_function(
    function_name="get_weather",
    handler=handle_weather,
    cancel_on_interruption=True
)
```

<ParamField path="function_name" type="Optional[str]" required>
  The name of the function to handle. Use None for a catch-all handler that
  processes all function calls.
</ParamField>

<ParamField path="handler" type="Callable" required>
  The async function handler. Should accept a single `FunctionCallParams`
  parameter.
</ParamField>

<ParamField path="start_callback" type="Optional[Callable]" deprecated>
  Legacy callback function (deprecated). Put initialization code at the top of
  your handler instead.
</ParamField>

<ParamField path="cancel_on_interruption" type="bool" default="True">
  Whether to cancel this function call when a user interruption occurs.
</ParamField>

## Usage Examples

### Basic LLM Switching

```python
from pipecat.pipeline.llm_switcher import LLMSwitcher
from pipecat.pipeline.service_switcher import ServiceSwitcherStrategyManual
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.frames.frames import ManuallySwitchServiceFrame

# Create LLM services
openai = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
gemini = GoogleLLMService(api_key=os.getenv("GOOGLE_API_KEY"))

# Create switcher
llm_switcher = LLMSwitcher(
    llms=[openai, gemini],
    strategy_type=ServiceSwitcherStrategyManual
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm_switcher,
    tts,
    transport.output(),
    context_aggregator.assistant()
])

# Switch to cheaper model for simple tasks
await task.queue_frame(ManuallySwitchServiceFrame(service=gpt35))
```
