---
title: "Building with Gemini Live"
description: "Create a real-time AI chatbot using Gemini Live and Pipecat"
---

<Tip>
**Gemini x Pipecat Hackathon 2025**

For the most up to date getting started information, check out the [Hackathon guide](https://docs.google.com/document/d/1B89fAVXQwPrLuy8-hee2h3WSndx5CM_Vve-LOZXZkL0/edit?tab=t.0#heading=h.naqf3pekw46) for starter projects.

To learn the basics, check out the following starter projects:

- Twilio phone bot: [pcc-gemini-twilio](https://github.com/daily-co/pcc-gemini-twilio)
- Web + screensharing bot: [pcc-gemini-screen-voice-ui-kit](https://github.com/daily-co/pcc-gemini-screen-voice-ui-kit)

</Tip>

This guide will walk you through building a real-time AI chatbot using Gemini Live and Pipecat. We'll create a complete application with a Pipecat server and a Pipecat React client that enables natural conversations with an AI assistant.

<Frame>![Pipecat + Gemini Live](/images/gemini-client-final.png)</Frame>

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="book"
    href="/server/services/s2s/gemini-live"
  >
    Gemini Live API documentation
  </Card>
  <Card
    title="Example Code"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"
  >
    Find the complete client and server code in Github
  </Card>
  <Card title="Client SDK" icon="react" href="/client/react/introduction">
    Pipecat React SDK documentation
  </Card>
</CardGroup>

## What We'll Build

In this guide, you'll create:

- A FastAPI server that manages bot instances
- A Gemini-powered conversational AI bot
- A React client with real-time audio/video
- A complete pipeline for speech-to-speech interaction

## Key Concepts

Before we dive into implementation, let's cover some important concepts that will help you understand how Pipecat and Gemini work together.

### Understanding Pipelines

At the heart of Pipecat is the pipeline system. A pipeline is a sequence of processors that handle different aspects of the conversation flow. Think of it like an assembly line where each station (processor) performs a specific task.

For our chatbot, the pipeline looks like this:

```python
pipeline = Pipeline([
    transport.input(),              # Receives audio/video from the user via WebRTC
    rtvi,                           # Handles client/server messaging and events
    context_aggregator.user(),      # Manages user message history
    llm,                            # Processes speech through Gemini
    talking_animation,              # Controls bot's avatar
    transport.output(),             # Sends audio/video back to the user via WebRTC
    context_aggregator.assistant(), # Manages bot message history
])
```

### Processors

Each processor in the pipeline handles a specific task:

<CardGroup cols={2}>
  <Card title="Transport" icon="arrow-right-arrow-left">
    `transport.input()` and `transport.output()` handle media streaming with
    Daily
  </Card>

<Card title="Context" icon="brain">
  `context_aggregator` maintains conversation history for natural dialogue
</Card>

<Card title="Speech Processing" icon="waveform-lines">
  `rtvi_user_transcription` and `rtvi_bot_transcription` handle speech-to-text
</Card>

  <Card title="Animation" icon="robot">
    `talking_animation` controls the bot's visual state based on speaking
    activity
  </Card>
</CardGroup>

<Note>
The order of processors matters! Data flows through the pipeline in sequence,
so each processor should receive the data it needs from previous processors.

Learn more about the [Core Concepts](/guides/learn/overview) to Pipecat server.

</Note>

### Gemini Integration

The `GeminiLiveLLMService` is a speech-to-speech LLM service that interfaces with the Gemini Live API.

It provides:

- Real-time speech-to-speech conversation
- Context management
- Voice activity detection
- Tool use

<Note>
  Pipecat manages two types of connections:
  1. A WebRTC connection between the Pipecat client and server for reliable audio/video streaming
  2. A WebSocket connection between the Pipecat server and Gemini for real-time AI processing

This architecture ensures stable media streaming while maintaining responsive AI interactions.

</Note>

## Prerequisites

Before we begin, you'll need:

- Python 3.10 or higher
- Node.js 16 or higher
- A [Daily API key](https://dashboard.daily.co/u/signup?pipecat=1)
- A Google API key with Gemini Live access
- Clone the Pipecat repo:

```bash
git clone git@github.com:pipecat-ai/pipecat.git
```

## Server Implementation

Let's start by setting up the server components. Our server will handle bot management, room creation, and client connections.

### Environment Setup

1. Navigate to the simple-chatbot's server directory:

```bash
cd examples/simple-chatbot/server
```

2. Set up a python virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install requirements:

```bash
pip install -r requirements.txt
```

4. Copy env.example to .env and make a few changes:

```bash
# Remove the hard-coded example room URL
DAILY_SAMPLE_ROOM_URL=

# Add your Daily and Gemini API keys
DAILY_API_KEY=[your key here]
GEMINI_API_KEY=[your key here]

# Use Gemini implementation
BOT_IMPLEMENTATION=gemini
```

### Server Setup (server.py)

`server.py` is a FastAPI server that creates the meeting room where clients and bots interact, manages bot instances, and handles client connections. It's the orchestrator that brings everything on the server-side together.

#### Creating Meeting Room

The server uses Daily's API via a [REST API helper](/server/utilities/daily/rest-helper#create-room) to create rooms where clients and bots can meet. Each room is a secure space for audio/video communication:

```python server/server.py
async def create_room_and_token():
    """Create a Daily room and generate access credentials."""
    room = await daily_helpers["rest"].create_room(DailyRoomParams())
    token = await daily_helpers["rest"].get_token(room.url)
    return room.url, token
```

#### Managing Bot Instances

When a client connects, the server starts a new bot instance configured specifically for that room. It keeps track of running bots and ensures there's only one bot per room:

```python server/server.py
# Start the bot process for a specific room
bot_file = "bot-gemini.py"
proc = subprocess.Popen([f"python3 -m {bot_file} -u {room_url} -t {token}"])
bot_procs[proc.pid] = (proc, room_url)
```

#### Connection Endpoints

The server provides two ways to connect:

<CardGroup cols={2}>
  <Card title="Browser Access (/)" icon="browser">
    Creates a room, starts a bot, and redirects the browser to the Daily meeting
    URL. Perfect for quick testing and development.
  </Card>

  <Card title="RTVI Client (/connect)" icon="plug">
    Creates a room, starts a bot, and returns connection credentials. Used by
    RTVI clients for custom implementations.
  </Card>
</CardGroup>

### Bot Implementation (bot-gemini.py)

The bot implementation connects all the pieces: Daily transport, Gemini service, conversation context, and processors.

Let's break down each component:

#### Transport Setup

First, we configure the Daily transport, which handles WebRTC communication between the client and server.

```python server/bot-gemini.py
transport = DailyTransport(
    room_url,
    token,
    "Chatbot",
    DailyParams(
        audio_in_enabled=True,        # Enable audio input
        audio_out_enabled=True,       # Enable audio output
        video_out_enabled=True,      # Enable video output
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.5)),
    ),
)
```

<Info>
Gemini Live audio requirements:

- Input: 16 kHz sample rate
- Output: 24 kHz sample rate

</Info>

#### Gemini Service Configuration

Next, we initialize the Gemini service which will provide speech-to-speech inference and communication:

```python server/bot-gemini.py
llm = GeminiLiveLLMService(
    api_key=os.getenv("GEMINI_API_KEY"),
    voice_id="Puck",                     # Choose your bot's voice
    params=InputParams(temperature=0.7)  # Set model input params
)
```

#### Conversation Context

We give our bot its personality and initial instructions:

```python server/bot-gemini.py
messages = [{
    "role": "user",
    "content": """You are Chatbot, a friendly, helpful robot.
                 Keep responses brief and avoid special characters
                 since output will be converted to audio."""
}]

context = LLMContext(messages)
context_aggregator = LLMContextAggregatorPair(context)
```

<Note>
  The context aggregator automatically maintains conversation history, helping
  the bot remember previous interactions.
</Note>

#### Processor Setup

We initialize two additional processors in our pipeline to handle different aspects of the interaction:

<CardGroup cols={2}>
  <Card title="RTVI Processors" icon="message-bot">
    `RTVIProcessor`: Handles all client communication events including transcriptions,
    speaking states, and performance metrics
  </Card>

  <Card title="Animation" icon="robot">
    `TalkingAnimation`: Controls the bot's visual state, switching between
    static and animated frames based on speaking status
  </Card>
</CardGroup>

<Info>
  [Learn more](/server/frameworks/rtvi/introduction) about the RTVI framework
  and available processors.
</Info>

#### Pipeline Assembly

Finally, we bring everything together in a pipeline:

```python server/bot-gemini.py
pipeline = Pipeline([
    transport.input(),             # Receive media
    rtvi,                          # Client UI events
    context_aggregator.user(),     # Process user context
    llm,                           # Gemini processing
    ta,                            # Animation (talking/quiet states)
    transport.output(),            # Send media
    context_aggregator.assistant() # Process bot context
])

task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
        enable_usage_metrics=True,
    ),
    observers=[RTVIObserver(rtvi)],
)
```

<Tip>
  The order of processors is crucial! For example, the RTVI processor should be 
  early in the pipeline to capture all relevant events.

The `RTVIObserver` monitors the entire pipeline and automatically collects relevant events to send to the client.

</Tip>

## Client Implementation

Our React client uses the [Pipecat React SDK](/client/react/introduction) to communicate with the bot. Let's explore how the client connects and interacts with our Pipecat server.

### Connection Setup

The client needs to connect to our bot server using the same transport type (Daily WebRTC) that we configured on the server:

```typescript examples/react/src/providers/PipecatProvider.tsx
const client = new PipecatClient({
  transport: new DailyTransport(),
  enableMic: true, // Enable audio input
  enableCam: false, // Disable video input
  enableScreenShare: false, // Disable screen sharing
});
client.startBotAndConnect({
  endpoint: "http://localhost:7860/connect", // Your bot connection endpoint
});
```

The connection configuration must match your server:

- `DailyTransport`: Matches the WebRTC transport used in `bot-gemini.py`
- `connect` endpoint: Matches the `/connect` route in `server.py`
- Media settings: Controls which devices are enabled on join

### Media Handling

Pipecat's React components handle all the complex media stream management for you:

```tsx
function App() {
  return (
    <PipecatClientProvider client={client}>
      <div className="app">
        <PipecatClientVideo participant="bot" /> {/* Bot's video feed */}
        <PipecatClientAudio /> {/* Audio input/output */}
      </div>
    </PipecatClientProvider>
  );
}
```

The `PipecatClientProvider` is the root component for providing Pipecat client context to your application. By wrapping your `PipecatClientAudio` and `PipecatClientVideo` components in this provider, they can access the client instance and receive and process the streams received from the Pipecat server.

### Real-time Events

The RTVI processors we configured in the pipeline emit events that we can handle in our client:

```typescript
// Listen for transcription events
useRTVIClientEvent(RTVIEvent.UserTranscript, (data: TranscriptData) => {
  if (data.final) {
    console.log(`User said: ${data.text}`);
  }
});

// Listen for bot responses
useRTVIClientEvent(RTVIEvent.BotOutput, (data: BotOutputData) => {
  if (data.aggregated_by === "sentence") {
    console.log(`Bot responded: ${data.text}`);
  }
});
```

<CardGroup cols={2}>
  <Card title="Available Events" icon="bell">
  - Speaking state changes
  - Transcription updates
  - Bot responses
  - Connection status
  - Performance metrics

  </Card>

  <Card title="Event Usage" icon="code">
  
  Use these events to:
  - Show speaking indicators
  - Display transcripts
  - Update UI state
  - Monitor performance

  </Card>

</CardGroup>

<Tip>
  Optionally, uses callbacks to handle events in your application. [Learn
  more](/client/js/api-reference/callbacks#callbacks) in the Pipecat client
  docs.
</Tip>

### Complete Example

Here's a basic client implementation with connection status and transcription display:

```tsx
function ChatApp() {
  return (
    <PipecatClientProvider client={client}>
      <div className="app">
        {/* Connection UI */}
        <StatusDisplay />
        <ConnectButton />

        {/* Media Components */}
        <BotVideo />
        <PipecatClientAudio />

        {/* Debug/Transcript Display */}
        <DebugDisplay />
      </div>
    </PipecatClientProvider>
  );
}
```

<Note>
  Check out the [example
  repository](https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot)
  for a complete client implementation with styling and error handling.
</Note>

## Running the Application

From the `simple-chatbot` directory, start the server and client to test the chatbot:

### 1. Start the Server

In one terminal:

```bash
python server/server.py
```

### 2. Start the Client

In another terminal:

```bash
cd examples/react
npm install
npm run dev
```

### 3. Testing the Connection

1. Open `http://localhost:5173` in your browser
2. Click "Connect" to join a room
3. Allow microphone access when prompted
4. Start talking with your AI assistant

<Warning>
Troubleshooting:
- Check that all API keys are properly configured in .env
- Grant your browser access to your microphone, so it can receive your audio input
- Verify WebRTC ports aren't blocked by firewalls

</Warning>

## Next Steps

Now that you have a working chatbot, consider these enhancements:

- Add custom avatar animations
- Implement [function calling](/guides/learn/function-calling) for external integrations
- Add support for multiple languages
- Enhance error recovery and reconnection logic

### Examples

<CardGroup cols={2}>
  <Card
    title="Foundational Example"
    icon="code"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/26a-gemini-live-transcription.py"
  >
    A basic implementation demonstrating core Gemini Live features and transcription capabilities
  </Card>

  <Card
    title="Simple Chatbot"
    icon="robot"
    href="https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot"
  >
    A complete client/server implementation showing how to build a Pipecat JS or React client that connects to a Gemini Live Pipecat bot
  </Card>
</CardGroup>

### Learn More

- [Gemini Live API Reference](/server/services/s2s/gemini-live)
- [React Client SDK Documentation](/client/react/introduction)
